---
layout: post
title:  "ARIMA, VAR, and LSTM"
category: [AI]
date:   2021-12-02 10:22:00 +0900
# prevPart: _posts/2021-11-30-traffic-prediction.md
published: false
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

For time series prediction, some of the baseline algorithms you can test are ARIMA, VAR, and LSTM. All are rather basic level of time series prediction models, and used for applications such as stock or traffic prediction.

[Cheetsheet for basic time-series forecasting methods](https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/)
# ARIMA

In statistics and econometrics, and in particular in time series analysis, an autoregressive integrated moving average (ARIMA) model is a generalization of an autoregressive moving average (ARMA) model.

Non-seasonal ARIMA models are generally denoted ARIMA(p,d,q) where parameters p, d, and q are non-negative integers, p is the order (number of time lags) of the autoregressive model, d is the degree of differencing (the number of times the data have had past values subtracted), and q is the order of the moving-average model.


Autoregressive integrated moving average ([ARIMA](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average)) is 


$$ {\displaystyle X_{t}-\alpha _{1}X_{t-1}-\dots -\alpha _{p'}X_{t-p'}=\varepsilon _{t}+\theta _{1}\varepsilon _{t-1}+\cdots +\theta _{q}\varepsilon _{t-q},} $$

Useful materials:
* <https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/>
* {% reference barberBRML2012 %} Chapter 24: Continuous-state Markov Models.

```python
# ARIMA example
from statsmodels.tsa.arima.model import ARIMA
from random import random
# contrived dataset
data = [x + random() for x in range(1, 100)]
# fit model
model = ARIMA(data, order=(3, 1, 1))
model_fit = model.fit()
# make prediction
yhat = model_fit.predict(len(data), len(data), typ='levels')
print(yhat)
```


# VAR

Vector autoregression ([VAR](https://en.wikipedia.org/wiki/Vector_autoregression))

$$ y_t = c + A_1 y_{t-1} + A_2 y_{t-2} + ... + A_p y_{t-p} + e_t $$

while the $$ y_t \in \mathbb{R}^k $$ which is a $$k$$-dimensional vector.

Useful articles:
* <https://www.machinelearningplus.com/time-series/vector-autoregression-examples-python/>

```python
# VAR example
from statsmodels.tsa.vector_ar.var_model import VAR
from random import random
# contrived dataset with dependency
data = list()
for i in range(100):
    v1 = i + random()
    v2 = v1 + random()
    row = [v1, v2]
    data.append(row)
# fit model
model = VAR(data)
model_fit = model.fit()
# make prediction
yhat = model_fit.forecast(model_fit.y, steps=1)
print(yhat)
```

# LSTM

<p align='center'><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/450px-LSTM_Cell.svg.png" width=400></p>

Long short-term memory ([LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)) is a recurrent network cell for sequence prediction.

### LSTM with a forget gate

$$ {\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}x_{t}+U_{f}h_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}x_{t}+U_{i}h_{t-1}+b_{i})\\o_{t}&=\sigma _{g}(W_{o}x_{t}+U_{o}h_{t-1}+b_{o})\\{\tilde {c}}_{t}&=\sigma _{c}(W_{c}x_{t}+U_{c}h_{t-1}+b_{c})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ {\tilde {c}}_{t}\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}} $$


```python
import tensorflow as tf

inputs = tf.keras.Input(shape=(10, 8))
lstm = tf.keras.layers.LSTM(4)
output = lstm(inputs)
print(output.shape)

# You can choose to return in sequences or a value
lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)
whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)
print(whole_seq_output.shape)
print(final_memory_state.shape)
print(final_carry_state.shape)

model =  tf.keras.Model(inputs, whole_seq_output)
model.compile(optimizer='adam', loss='mse')
model.fit(np.random.random((3200, 10, 8)), np.random.random((3200, 10, 4)), epochs=3)
```